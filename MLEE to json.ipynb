{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import os\n",
    "import stanza\n",
    "from stanza import Pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = './stanfordmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Pipeline(dir=MODELS_DIR,processors='tokenize,pos,depparse,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_path = './dataset/train/entity'\n",
    "event_path = './dataset/train/event'\n",
    "text_path = './dataset/train/text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_set = ['Dephosphorylation','Binding','Blood_vessel_development','Breakdown',\n",
    "               'Catabolism','Cell_proliferation','Death','Development','Gene_expression',\n",
    "               'Growth','Localization','Negative_regulation','Positive_regulation',\n",
    "               'Phosphorylation','Planned_process','Regulation','Remodeling','Synthesis',\n",
    "               'Transcription']\n",
    "\n",
    "role_set = ['Cause','Theme','Theme2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all of filename in the folder\n",
    "def getFileNames(path1,path2,path3):\n",
    "    entity_dir = os.listdir(path1)\n",
    "    event_dir = os.listdir(path2)\n",
    "    text_dir = os.listdir(path3)\n",
    "    \n",
    "    return sorted(entity_dir),sorted(event_dir),sorted(text_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_textFile(path):\n",
    "    with codecs.open(path,'r',encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "    text = []\n",
    "    for line in content:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        text.append(line.strip())\n",
    "    \n",
    "    text_list = []\n",
    "    for i,t in enumerate(text):\n",
    "        if i==0:\n",
    "            text_list.append(t)\n",
    "        else:\n",
    "            t=' '+t\n",
    "            t_ = t.split('.')[:-1]\n",
    "            for l in t_:\n",
    "                l=l+'.'\n",
    "                text_list.append(l)\n",
    "    \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_entityFile(path):\n",
    "    entity_list = [line.strip().split('\\t') for line in codecs.open(path,'r',encoding='utf-8').readlines()]\n",
    "    \n",
    "    entity_dic = {}\n",
    "    for entity in entity_list:\n",
    "        key = entity[0]\n",
    "        value = {}\n",
    "        infos = entity[1].split(' ')\n",
    "        entity_type = infos[0]\n",
    "        entity_ch_st = infos[1]\n",
    "        entity_ch_ed = infos[2]\n",
    "        value['entity_type'] = entity_type\n",
    "        value['entity_ch_st'] = entity_ch_st\n",
    "        value['entity_ch_ed'] = entity_ch_ed\n",
    "        entity_text = entity[2]\n",
    "        value['entity_text'] = entity_text\n",
    "        entity_dic[key] = value\n",
    "    \n",
    "    return entity_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_eventFile(path):\n",
    "    tri_event_list = [line.strip().split('\\t') for line in codecs.open(path,'r',encoding='utf-8').readlines()]\n",
    "    \n",
    "    event_dic = {}\n",
    "    trigger_dic = {}\n",
    "    \n",
    "    for item in tri_event_list:\n",
    "        tag = item[0]\n",
    "        if tag.startswith('T'):\n",
    "            trigger_dic[tag] = {}\n",
    "            temp = item[1].split()\n",
    "            trigger_dic[tag]['trigger_ch_st'] = temp[1]\n",
    "            trigger_dic[tag]['trigger_ch_ed'] = temp[2]\n",
    "            trigger_dic[tag]['trigger_text'] = item[2]\n",
    "        elif tag.startswith('E'):\n",
    "            event_dic[tag] = {}\n",
    "            temp = item[1].split()\n",
    "            event_dic[tag]['event_type'] = temp[0].split(':')[0]\n",
    "            event_dic[tag]['trigger'] = temp[0].split(':')[1]\n",
    "            lens = len(temp)\n",
    "            if lens == 1:\n",
    "                event_dic[tag]['arguments'] = ['NONE']\n",
    "            else:\n",
    "                event_dic[tag]['arguments'] = []\n",
    "                for arg in temp[1:]:\n",
    "                    arg_role = arg.split(':')[0]\n",
    "                    arg_tag = arg.split(':')[-1]\n",
    "                    value = {}\n",
    "                    value['arg_role'] = arg_role\n",
    "                    value['arg_tag'] = arg_tag\n",
    "                    event_dic[tag]['arguments'].append(value)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return trigger_dic,event_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punk = ['.',',',':','!','?','%','(',')','<','>','{','}','[',']','\\'','\\\"']\n",
    "special_punk = ['-','/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_local(text_index,text_list,ch_s,ch_e):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    doc = tokenize(text_list[text_index].strip())\n",
    "    word_li = [word.text for sent in doc.sentences for word in sent.words]\n",
    "    if text_index == 0:\n",
    "        st_i = 0\n",
    "        ed_i = 0\n",
    "        length = 0\n",
    "        punk_num = 0\n",
    "        for index,word in enumerate(word_li):\n",
    "            pre_length = length\n",
    "            length +=len(word)\n",
    "            if word in punk:\n",
    "                punk_num += 1\n",
    "            if word in special_punk:\n",
    "                punk_num += 2\n",
    "                \n",
    "            st_i = pre_length+index-punk_num\n",
    "            if st_i == ch_s:\n",
    "                start = index\n",
    "                length = 0\n",
    "                punk_num = 0\n",
    "                break\n",
    "        \n",
    "        for index,word in enumerate(word_li):\n",
    "            length += len(word)\n",
    "            if word in punk:\n",
    "                punk_num += 1\n",
    "            if word in special_punk:\n",
    "                punk_num += 2\n",
    "            \n",
    "            ed_i = length+index-punk_num\n",
    "            if ed_i ==ch_e:\n",
    "                end = index+1\n",
    "                break\n",
    "    else:\n",
    "        basic_len = 0\n",
    "        for i in range(text_index):\n",
    "            basic_len += len(text_list[i])\n",
    "        st_i = 0\n",
    "        ed_i = 0\n",
    "        length = basic_len+1\n",
    "        punk_num = 0\n",
    "        for index,word in enumerate(word_li):\n",
    "            pre_lenth = length\n",
    "            length += len(word)\n",
    "            if word in punk:\n",
    "                punk_num += 1\n",
    "            if word in special_punk:\n",
    "                punk_num += 2\n",
    "            \n",
    "            st_i = pre_lenth+index-punk_num\n",
    "            if st_i == ch_s:\n",
    "                start = index\n",
    "                length = basic_len+1\n",
    "                punk_num = 0\n",
    "                break\n",
    "                \n",
    "        \n",
    "        for index,word in enumerate(word_li):\n",
    "            length += len(word)\n",
    "            if word in punk:\n",
    "                punk_num += 1\n",
    "            if word in special_punk:\n",
    "                punk_num += 2\n",
    "            \n",
    "            ed_i = length+index-punk_num\n",
    "            if ed_i ==ch_e:\n",
    "                end = index+1\n",
    "                break\n",
    "                \n",
    "    return start,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_entityDic(dic_,text_list):\n",
    "    entity_dic = dic_\n",
    "    for key,value in entity_dic.items():\n",
    "        ch_st = int(value['entity_ch_st'])\n",
    "        ch_ed = int(value['entity_ch_ed'])\n",
    "        entity_text = value['entity_text']\n",
    "        temp = ''\n",
    "        for i,text in enumerate(text_list):\n",
    "            temp = temp+text\n",
    "            if len(temp) >=ch_ed-1:\n",
    "                if i != 0:\n",
    "                    ch_st = ch_st-1\n",
    "                    ch_ed = ch_ed -1\n",
    "                if temp[ch_st:ch_ed] == entity_text:\n",
    "                    if 'text_index' in entity_dic[key]:\n",
    "                        break;\n",
    "                    entity_dic[key]['text_index'] = i\n",
    "                    start,end = find_local(i,text_list,ch_st,ch_ed)\n",
    "                    entity_dic[key]['start'] = start\n",
    "                    entity_dic[key]['end'] = end\n",
    "    \n",
    "    return entity_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_triggerDic(dic_,text_list):\n",
    "    trigger_dic = dic_\n",
    "    for key,value in trigger_dic.items():\n",
    "        ch_st = int(value['trigger_ch_st'])\n",
    "        ch_ed = int(value['trigger_ch_ed'])\n",
    "        trigger_text = value['trigger_text']\n",
    "        temp = ''\n",
    "        for i,text in enumerate(text_list):\n",
    "            temp = temp+text\n",
    "            if len(temp) >=ch_ed-1:\n",
    "                if i != 0:\n",
    "                    ch_st = ch_st-1\n",
    "                    ch_ed = ch_ed -1\n",
    "                if temp[ch_st:ch_ed] == trigger_text:\n",
    "                    if 'text_index' in trigger_dic[key]:\n",
    "                        break;\n",
    "                    trigger_dic[key]['text_index'] = i\n",
    "                    start,end = find_local(i,text_list,ch_st,ch_ed)\n",
    "                    trigger_dic[key]['start'] = start\n",
    "                    trigger_dic[key]['end'] = end\n",
    "    \n",
    "    return trigger_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_eventDic(dic_,tri_dic):\n",
    "    event_dic = dic_\n",
    "    for key,value in event_dic.items():\n",
    "        trigger_key = value['trigger']\n",
    "        event_dic[key]['trigger'] = tri_dic[trigger_key]\n",
    "    \n",
    "    return event_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStanfordResult(text):\n",
    "    doc = tokenize(text)\n",
    "    word_list = [word.text for sent in doc.sentences for word in sent.words]\n",
    "    pos_list = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "    stanford_parse = [f\"{word.deprel}/dep={int(word.id)-1}/gov={int(word.head)-1}\" for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    return word_list,pos_list,stanford_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultJsonList(text_list,entity_dic,event_dic):\n",
    "    json_result = []\n",
    "    for i,sent in enumerate(text_list):\n",
    "        result_dic = {}\n",
    "        result_dic[\"sentence\"] = text_list[i].strip()\n",
    "        \n",
    "        word_list,pos_list,stanford_parse = getStanfordResult(text_list[i].strip())\n",
    "        result_dic[\"tokens\"] = word_list\n",
    "        result_dic[\"pos-tags\"] = pos_list\n",
    "        result_dic[\"parse\"] = stanford_parse\n",
    "        \n",
    "        gold_entity_mentions = []\n",
    "        for key,value in entity_dic.items():\n",
    "            en_dic = {}\n",
    "            if value['text_index'] == i:\n",
    "                en_dic[\"text\"] = value['entity_text']\n",
    "                en_dic[\"start\"] = value['start']\n",
    "                en_dic[\"end\"] = value['end']\n",
    "                en_dic[\"entity_type\"] = value['entity_type']\n",
    "                gold_entity_mentions.append(en_dic)\n",
    "        result_dic[\"gold_entity_mentions\"] = gold_entity_mentions\n",
    "\n",
    "        gold_event_mentions = []\n",
    "        for key,value in event_dic.items():\n",
    "            even_dic = {}\n",
    "            if value['trigger']['text_index'] == i:\n",
    "                event_type = value['event_type']\n",
    "                if event_type in trigger_set:\n",
    "                    trigger = {}\n",
    "                    trigger[\"text\"] = value['trigger']['trigger_text']\n",
    "                    trigger[\"start\"] = value['trigger']['start']\n",
    "                    trigger[\"end\"] = value['trigger']['end']\n",
    "                    even_dic[\"trigger\"] = trigger\n",
    "\n",
    "                    arguments = []\n",
    "                    if value['arguments'] == ['NONE']:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for arg in value['arguments']:\n",
    "                            role = arg['arg_role']\n",
    "                            if role in role_set:\n",
    "                                arg_info = {}\n",
    "                                arg_info[\"role\"] = role\n",
    "                                tag = arg['arg_tag']\n",
    "                                if tag.startswith('T'):\n",
    "                                    arg_info[\"text\"] = entity_dic[tag]['entity_text']\n",
    "                                    arg_info[\"start\"] = entity_dic[tag]['start']\n",
    "                                    arg_info[\"end\"] = entity_dic[tag]['end']\n",
    "                                else:\n",
    "                                    tri = event_dic[tag]['trigger']\n",
    "                                    arg_info[\"text\"] = tri['trigger_text']\n",
    "                                    arg_info[\"start\"] = tri['start']\n",
    "                                    arg_info[\"end\"] = tri['end']\n",
    "                                arguments.append(arg_info)\n",
    "                    even_dic[\"arguments\"] = arguments\n",
    "                    even_dic[\"event_type\"] = event_type\n",
    "                    gold_event_mentions.append(even_dic)\n",
    "        result_dic[\"gold_event_mentions\"] = gold_event_mentions\n",
    "        json_result.append(result_dic)\n",
    "        \n",
    "        return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeJson(path,jsonlist):\n",
    "    jsonstr = json.dumps(jsonlist,indent=8,ensure_ascii=False)\n",
    "    print(jsonstr)\n",
    "    \n",
    "    with codecs.open(path,'w',encoding='utf-8') as f:\n",
    "        f.write(jsonstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_dir,event_dir,text_dir = getFileNames(entity_path,event_path,text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_result = []\n",
    "for entity_fname,event_fname,text_fname in tqdm(zip(entity_dir,event_dir,text_dir)):\n",
    "    entity_fpath = os.path.join(entity_path,entity_fname)\n",
    "    event_fpath = os.path.join(event_path,event_fname)\n",
    "    text_fpath = os.path.join(text_path,text_fname)\n",
    "    \n",
    "    text_list = read_textFile(text_fpath)\n",
    "    entity_dic = read_entityFile(entity_fpath)\n",
    "    trigger_dic,event_dic = read_eventFile(event_fpath)\n",
    "    \n",
    "    entity_dic = reconstruct_entityDic(entity_dic,text_list)\n",
    "    trigger_dic = reconstruct_triggerDic(trigger_dic,text_list)\n",
    "    event_dic = reconstruct_eventDic(event_dic,trigger_dic)\n",
    "    \n",
    "    try:\n",
    "        result = getResultJsonList(text_list,entity_dic,event_dic)\n",
    "    except KeyError:\n",
    "        print(entity_fname,event_fname,text_fname)\n",
    "    json_result.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeJson(path='train.json',jsonlist=json_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
