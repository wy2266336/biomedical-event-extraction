{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.utils import data\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torchcrf import CRF\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "from seqeval.metrics import accuracy_score,f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import transformers\n",
    "from transformers import BertModel,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal loss and Dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-classification Focal Loss\n",
    "def focal_loss(input,target,alpha=0.5,gamma=2,weight=None,ignore_index=-100,reduction='mean'):\n",
    "    '''\n",
    "    reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed\n",
    "    '''\n",
    "    logpt = -F.nll_loss(input=input,target=target,weight=weight,ignore_index=ignore_index,reduction=reduction)\n",
    "    pt = torch.exp(logpt)\n",
    "    loss = -((1-pt)**gamma)*alpha*logpt\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-classification Dice Loss\n",
    "def dice_loss(input,target,esp=1e-5,alpha=0.2,loss_type='DSC'):\n",
    "    '''\n",
    "    :param input: floattensor, the result of softmax layer, shape is (batch_size,class_num)\n",
    "    :param target: longtensor, the true label, shape is (batch_size)\n",
    "    :param esp: float,for smoothing purposes, default is 1e-5\n",
    "    :param alpha: float,it will be used if loss_type is 'TL', and range is (0,1)\n",
    "    :loss_type: str, there are 4 kinds of dice loss: 'DSC','DL','TL' and 'sadDSC'\n",
    "    '''\n",
    "    \n",
    "    loss = 0\n",
    "    B = input.shape[0]\n",
    "    C = input.shape[1]\n",
    "    \n",
    "    target = target.unsqueeze(dim=1)\n",
    "    class_mask = input.data.new_zeros(B,C)\n",
    "    class_mask.scatter_(1,target,1.)\n",
    "    \n",
    "    numerator = input*class_mask\n",
    "    if loss_type=='DSC':\n",
    "        denominator = input + class_mask\n",
    "        loss_tensor = (2*numerator+esp)/(denominator+esp)\n",
    "        loss = loss_tensor.sum()\n",
    "    elif loss_type=='DL':\n",
    "        denominator = input**2 + class_mask**2\n",
    "        loss_tensor = (2*numerator+esp)/(denominator+esp)\n",
    "        loss = loss_tensor.sum()\n",
    "    elif loss_type=='TL':\n",
    "        denominator = numerator+alpha*input*(1-class_mask)+(1-alpha)*(1-input)*class_mask\n",
    "        loss_tensor = (numerator+esp)/(denominator+esp)\n",
    "        loss = loss_tensor.sum()\n",
    "    else:\n",
    "        numerator = (1-input)*numerator\n",
    "        denominator = (1-input)*input+class_mask\n",
    "        loss_tensor = (2*numerator+esp)/(denominator+esp)\n",
    "        loss = loss_tensor.sum()\n",
    "    loss = 1-loss/B\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottle(nn.Module):\n",
    "    ''' Perform the reshape routine before and after an operation '''\n",
    "\n",
    "    def forward(self, input):\n",
    "        if len(input.size()) <= 2:\n",
    "            return super(Bottle, self).forward(input)\n",
    "        size = input.size()[:2]\n",
    "        out = super(Bottle, self).forward(torch.reshape(input,(size[0] * size[1], -1)))\n",
    "        return out.view(size[0], size[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierLinear(nn.Module):\n",
    "    '''\n",
    "    Simple Linear layer with Xavier init\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(XavierLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(OrthogonalLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        nn.init.orthogonal_(self.linear.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottledXavierLinear(Bottle, XavierLinear):\n",
    "    pass\n",
    "\n",
    "class BottledOrthogonalLinear(Bottle, OrthogonalLinear):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, inputs, outputs, edge_types, dropout=0.5, bias=True, use_bn=False, device=torch.device(\"cpu\")):\n",
    "        \"\"\"\n",
    "        Single Layer GraphConvolution\n",
    "\n",
    "        :param inputs: The number of incoming features\n",
    "        :param outputs: The number of output features\n",
    "        :param edge_types: The number of edge types in the whole graph\n",
    "        :param dropout: Dropout keep rate, if not bigger than 0, 0 or None, default 0.5\n",
    "        :param bias: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.edge_types = edge_types\n",
    "        self.dropout = dropout if type(dropout) == float and -1e-7 < dropout < 1 + 1e-7 else None\n",
    "        \n",
    "        self.Weight = Parameter(torch.Tensor(self.inputs,self.outputs))\n",
    "        #parameters for gates\n",
    "        self.Gates = nn.ModuleList()\n",
    "        \n",
    "        #parameters for graph convolutions\n",
    "        self.GraphConv = nn.ModuleList()\n",
    "        \n",
    "        #batch norm\n",
    "        self.use_bn = use_bn\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.BatchNorm1d(self.outputs)\n",
    "            \n",
    "        for _ in range(edge_types):\n",
    "            self.Gates.append(BottledOrthogonalLinear(in_features=inputs,out_features=1,bias=bias))\n",
    "            self.GraphConv.append(BottledOrthogonalLinear(in_features=inputs,out_features=outputs,bias=bias))\n",
    "        \n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, inputs, adj):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: FloatTensor, input feature tensor, (batch_size, seq_len, hidden_size)\n",
    "        :param adj: FloatTensor (sparse.FloatTensor.to_dense()), adjacent matrix for provided graph of padded sequences, (batch_size, edge_types, seq_len, seq_len)\n",
    "        :return: output\n",
    "            - **output**: FloatTensor, output feature tensor with the same size of input, (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        adj_ = adj.transpose(0, 1)  # (edge_types, batch_size, seq_len, seq_len)\n",
    "        ts = []\n",
    "        \n",
    "        inputs = torch.matmul(inputs,self.Weight)\n",
    "        for i in range(self.edge_types):\n",
    "            gate_status = F.relu(self.Gates[i](inputs))  # (batch_size, seq_len, 1) \n",
    "            adj_hat_i = adj_[i] * gate_status  # (batch_size, seq_len, seq_len)\n",
    "            ts.append(torch.bmm(adj_hat_i, self.GraphConv[i](inputs)))\n",
    "        ts = torch.stack(ts).sum(dim=0, keepdim=False).to(self.device)\n",
    "        if self.use_bn:\n",
    "            ts = ts.transpose(1, 2).contiguous()\n",
    "            ts = self.bn(ts)\n",
    "            ts = ts.transpose(1, 2).contiguous()\n",
    "        ts = F.relu(ts)\n",
    "        if self.dropout is not None:\n",
    "            ts = F.dropout(ts, p=self.dropout, training=self.training)\n",
    "        return ts\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.inputs) + ' -> ' + str(self.outputs) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynamicRNN model，including LSTM,GRU,RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, nonlinearity='relu', bias=True, batch_first=True,\n",
    "                dropout=0, bidirectional=True, rnn_mode='lstm', device=torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Dynamic RNN which can hold variable length sequence, and has different RNN mode,such as RNN,LSTM,GRU\n",
    "\n",
    "        :param input_size: The number of expected features in the input x\n",
    "        :param hidden_size: The number of features in the hidden state h\n",
    "        :param num_layers: Number of recurrent layers.\n",
    "        :param nonlinearity: The non-linearity to use. when the rnn_mode is 'rnn', Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "        :param bias: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, feature)\n",
    "        :param dropout: If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer\n",
    "        :param bidirectional: If True, becomes a bidirectional RNN. Default: False\n",
    "        :param rnn_mode: The different mode of RNN, can be either 'rnn','lstm' or 'gru'\n",
    "        \"\"\"\n",
    "        super(DynamicRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        if rnn_mode is 'rnn':\n",
    "            self.rnn = nn.RNN(input_size = input_size,\n",
    "                              hidden_size = hidden_size,\n",
    "                              num_layers = num_layers,\n",
    "                              nonlinearity = nonlinearity,\n",
    "                              bias = bias,\n",
    "                              batch_first = batch_first,\n",
    "                              dropout = dropout,\n",
    "                              bidirectional = bidirectional)\n",
    "        elif rnn_mode is 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size = input_size,\n",
    "                               hidden_size = hidden_size,\n",
    "                               num_layers = num_layers,\n",
    "                               bias = bias,\n",
    "                               batch_first = batch_first,\n",
    "                               dropout = dropout,\n",
    "                               bidirectional = bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size = input_size,\n",
    "                              hidden_size = hidden_size,\n",
    "                              num_layers = num_layers,\n",
    "                              bias = bias,\n",
    "                              batch_first = batch_first,\n",
    "                              dropout = dropout,\n",
    "                              bidirectional = bidirectional)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, input_x, x_len):\n",
    "        \"\"\"\n",
    "        sequence -> sort -> pad and pack -> process using RNN -> unpack -> unsort\n",
    "\n",
    "        :param input_x: FloatTensor, pre-padded input sequence (batch_size, seq_len, feature_dim)\n",
    "        :param x_len: numpy list, indicating corresponding actual sequence length\n",
    "        :return: output, h_n \n",
    "        - **output**: FloatTensor, packed output sequence (batch_size, seq_len, feature_dim * num_directions)\n",
    "            containing the output features `(h_t)` from the last layer of the RNN,LSTM or GRU, for each t.\n",
    "        - **h_n**: FloatTensor, (num_layers * num_directions, batch, hidden_size)\n",
    "            containing the hidden state for `t = seq_len`\n",
    "        \"\"\"\n",
    "        out_pack, h_n = self.rnn(input_x)\n",
    "        if isinstance(h_n,tuple):\n",
    "            h_n = h_n[0]\n",
    "        output = out_pack\n",
    "        \n",
    "        return output,h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, embedding_size=None, embedding_matrix=None,\n",
    "                 fine_tune=True, dropout=0.5,\n",
    "                 padding_idx=None,\n",
    "                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n",
    "                 sparse=False,\n",
    "                 device=torch.device(\"cpu\")):\n",
    "        '''\n",
    "        Embedding Layer need at least one of `embedding_size` and `embedding_matrix`\n",
    "        :param embedding_size: tuple, contains 2 integers indicating the shape of embedding matrix, eg: (20000, 300)\n",
    "        :param embedding_matrix: torch.Tensor, the pre-trained value of embedding matrix\n",
    "        :param fine_tune: boolean, whether fine tune embedding matrix\n",
    "        :param dropout: float, dropout rate\n",
    "        :param padding_idx: int, if given, pads the output with zeros whenever it encounters the index\n",
    "        :param max_norm: float, if given, will renormalize the embeddings to always have a norm lesser than this\n",
    "        :param norm_type: float, the p of the p-norm to compute for the max_norm option\n",
    "        :param scale_grad_by_freq: boolean, if given, this will scale gradients by the frequency of the words in the mini-batch\n",
    "        :param sparse: boolean, *unclear option copied from original module*\n",
    "        '''\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "            embedding_size = embedding_matrix.size()\n",
    "        else:\n",
    "            embedding_matrix = torch.nn.init.uniform_(torch.FloatTensor(embedding_size[0], embedding_size[1]),\n",
    "                                                      a=-0.15,\n",
    "                                                      b=0.15) \n",
    "                                                      \n",
    "        assert (embedding_size is not None)\n",
    "        assert (embedding_matrix is not None)\n",
    "        # Config copying\n",
    "        \n",
    "        self.matrix = nn.Embedding(num_embeddings=embedding_size[0],\n",
    "                                   embedding_dim=embedding_size[1],\n",
    "                                   padding_idx=padding_idx,\n",
    "                                   max_norm=max_norm,\n",
    "                                   norm_type=norm_type,\n",
    "                                   scale_grad_by_freq=scale_grad_by_freq,\n",
    "                                   sparse=sparse)\n",
    "        self.matrix.weight.data.copy_(embedding_matrix)\n",
    "        self.matrix.weight.requires_grad = fine_tune\n",
    "        self.dropout = dropout if type(dropout) == float and -1e-7 < dropout < 1 + 1e-7 else None\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward this module\n",
    "        :param x: torch.LongTensor, token sequence or sentence, shape is [batch, sentence_len]\n",
    "        :return: torch.FloatTensor, output data, shape is [batch, sentence_len, embedding_size]\n",
    "        '''\n",
    "        if self.dropout is not None:\n",
    "            return F.dropout(self.matrix(x), p=self.dropout, training=self.training)\n",
    "        else:\n",
    "            return self.matrix(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriggerEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,num_embedding,embedding_dim,padding_idx=None,device=torch.device(\"cpu\")):\n",
    "        super(TriggerEmbeddingLayer,self).__init__()\n",
    "        embedding_matrix = torch.nn.init.uniform_(torch.FloatTensor(num_embedding, embedding_dim),\n",
    "                                                  a=-0.15,\n",
    "                                                  b=0.15)\n",
    "        self.trigger = nn.Embedding(num_embeddings=num_embedding,embedding_dim=embedding_dim,padding_idx=padding_idx)\n",
    "        self.trigger.weight.data.copy_(embedding_matrix)\n",
    "        self.trigger.weight.requires_grad = True\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, trigger):\n",
    "        emb = self.trigger(trigger)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Label Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, embedding_size=None, embedding_matrix=None,\n",
    "                 fine_tune=True, dropout=0.5,\n",
    "                 padding_idx=None,\n",
    "                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n",
    "                 sparse=False,\n",
    "                 device=torch.device(\"cpu\")):\n",
    "        '''\n",
    "        MultiLabelEmbeddingLayer Layer need at least one of `embedding_size` and `embedding_matrix`\n",
    "        :param embedding_size: tuple, contains 2 integers indicating the shape of embedding matrix, eg: (20000, 300)\n",
    "        :param embedding_matrix: torch.Tensor, the pre-trained value of embedding matrix\n",
    "        :param fine_tune: boolean, whether fine tune embedding matrix\n",
    "        :param dropout: float, dropout rate\n",
    "        :param padding_idx: int, if given, pads the output with zeros whenever it encounters the index\n",
    "        :param max_norm: float, if given, will renormalize the embeddings to always have a norm lesser than this\n",
    "        :param norm_type: float, the p of the p-norm to compute for the max_norm option\n",
    "        :param scale_grad_by_freq: boolean, if given, this will scale gradients by the frequency of the words in the mini-batch\n",
    "        :param sparse: boolean, *unclear option copied from original module*\n",
    "        '''\n",
    "        super(MultiLabelEmbeddingLayer, self).__init__()\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "            embedding_size = embedding_matrix.size()\n",
    "        else:\n",
    "            embedding_matrix = torch.torch.randn(embedding_size[0], embedding_size[1])\n",
    "        assert (embedding_size is not None)\n",
    "        assert (embedding_matrix is not None)\n",
    "        # Config copying\n",
    "        self.matrix = nn.Embedding(num_embeddings=embedding_size[0],\n",
    "                                   embedding_dim=embedding_size[1],\n",
    "                                   padding_idx=padding_idx,\n",
    "                                   max_norm=max_norm,\n",
    "                                   norm_type=norm_type,\n",
    "                                   scale_grad_by_freq=scale_grad_by_freq,\n",
    "                                   sparse=sparse)\n",
    "        self.matrix.weight.data.copy_(embedding_matrix)\n",
    "        self.matrix.weight.requires_grad = fine_tune\n",
    "        self.dropout = dropout if type(dropout) == float and -1e-7 < dropout < 1 + 1e-7 else None\n",
    "\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward this module\n",
    "        :param x: list, token sequence or sentence, shape is [batch, sentence_len, variable_size(>=1)]\n",
    "        :return: torch.FloatTensor, output data, shape is [batch, sentence_len, embedding_size]\n",
    "        '''\n",
    "        BATCH = len(x)\n",
    "        SEQ_LEN = len(x[0])\n",
    "        x = [self.matrix(torch.LongTensor(x[i][j]).to(self.device)).sum(0)\n",
    "             for i in range(BATCH)\n",
    "             for j in range(SEQ_LEN)]\n",
    "        x = torch.stack(x).view(BATCH, SEQ_LEN, -1)\n",
    "        if self.dropout is not None:\n",
    "            return F.dropout(x, p=self.dropout, training=self.training)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self,embed_dim,num_heads,batch_first=False,dropout=0.0,bias=True,device=torch.device(\"cpu\")):\n",
    "        '''\n",
    "        :param embed_dim: total dimension of the model.\n",
    "        :param num_heads: parallel attention heads.\n",
    "        :param batch_first: If True, then the input and output tensors are provided as (batch, seq, embedding_dim)\n",
    "        :param dropout: a Dropout layer on attn_output_weights. Default: 0.0\n",
    "        :param bias: add bias as module parameter. Default: True\n",
    "        '''\n",
    "        super(MultiHeadAttentionLayer,self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim,\n",
    "                                                 num_heads=num_heads,\n",
    "                                                 dropout=dropout,\n",
    "                                                 bias=bias)\n",
    "        self.dense = nn.Linear(in_features=embed_dim,out_features=embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self,input_x,need_weights=True):\n",
    "        '''\n",
    "        :param input_x: input matrix, shape is (target_seq_len, batch, embed_dim)\n",
    "        :param need_weights: if True, the weight matrix will be output\n",
    "        \n",
    "        :return: attn_output: the shape is (target_seq_len, batch, embed_dim),if batch_first=True, the shape is (batch, target_seq_len, embed_dim)\n",
    "                 attn_output_weights: the shape is (batch,target_seq_len, source_seq_len)\n",
    "        '''\n",
    "        if self.batch_first:\n",
    "            input_x = torch.transpose(input_x,0,1)\n",
    "        Q = self.relu(self.dense(input_x))\n",
    "        K = self.relu(self.dense(input_x))\n",
    "        V = self.relu(self.dense(input_x))\n",
    "        \n",
    "        attn_output, attn_output_weights = self.attention(query=Q,key=K,value=V,need_weights=need_weights)\n",
    "        \n",
    "        if self.batch_first:\n",
    "            attn_output = torch.transpose(attn_output,0,1)\n",
    "        \n",
    "        return attn_output,attn_output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building BGModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGModel(nn.Module):\n",
    "    def __init__(self,hyps,device=torch.device(\"cpu\"),embeddingMatrix=None):\n",
    "        super(BGModel,self).__init__()\n",
    "        self.hyperparams = copy.deepcopy(hyps)\n",
    "        self.device = device\n",
    "        self.bert=bert\n",
    "        #get bert token embedding dim\n",
    "        bert_embeddig_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        # POS-Tagging Layer\n",
    "        self.posembedding = EmbeddingLayer(embedding_size=(hyps['pos_size'],hyps['posemd_dim']),\n",
    "                                              dropout=hyps['pos_dp'],\n",
    "                                              device=device)\n",
    "        \n",
    "        # Entity-Label Layer\n",
    "        self.enembedding = MultiLabelEmbeddingLayer(embedding_size=(hyps['en_size'],hyps['enemb_dim']),\n",
    "                                                    dropout=hyps['en_dp'],\n",
    "                                                    device=device)\n",
    "        \n",
    "        self.BN = nn.BatchNorm1d(num_features=64).to(self.device)\n",
    "        #bidirectional dynamicRNN Layer\n",
    "        self.birnn = DynamicRNN(input_size=bert_embeddig_dim+hyps['posemd_dim']+hyps['enemb_dim'],\n",
    "                                hidden_size=hyps['rnn_dim'],\n",
    "                                num_layers=hyps['rnn_layers'],\n",
    "                                dropout=hyps['rnn_dp'],\n",
    "                                rnn_mode=hyps['rnn_mode'],\n",
    "                                device=device)\n",
    "        \n",
    "         \n",
    "    \n",
    "        \n",
    "        #GCN Layer\n",
    "        self.gcns = nn.ModuleList()\n",
    "        for i in range(hyps['gcn_layers']):\n",
    "            gcn = GraphConvolution(inputs=bert_embeddig_dim,\n",
    "                                   outputs=bert_embeddig_dim,\n",
    "                                   edge_types=hyps['gcn_et'],\n",
    "                                   dropout=hyps['gcn_dp'] if i != hyps[\"gcn_layers\"] - 1 else None,\n",
    "                                   use_bn=hyps['gcn_use_bn'],\n",
    "                                   device=device)\n",
    "            self.gcns.append(gcn)\n",
    "        \n",
    "        self.atten = MultiHeadAttentionLayer(embed_dim=bert_embeddig_dim,\n",
    "                                             num_heads=8,\n",
    "                                             batch_first=True,\n",
    "                                             dropout=0.5,\n",
    "                                             device=device)\n",
    "        \n",
    "        \n",
    "        #Tagger layer\n",
    "        self.tagger = nn.LSTM(input_size=2*hyps['rnn_dim']+bert_embeddig_dim,\n",
    "                              hidden_size=hyps['tag_hidden'],\n",
    "                              num_layers=1,\n",
    "                              bidirectional=False,\n",
    "                              batch_first=True).to(self.device)\n",
    "        \n",
    "        '''\n",
    "        trigger输出层和argument输出层属于Decoder层\n",
    "        '''\n",
    "        #Trigger embedding layer\n",
    "        self.triembedding = TriggerEmbeddingLayer(num_embedding=hyps['tri_size'],embedding_dim=hyps['tri_dim'],device=device)\n",
    "        #Trigger output layer\n",
    "        self.trigger_outlayer = BottledXavierLinear(in_features=hyps['tag_hidden'],out_features=triggers_size).to(self.device)\n",
    "        #Argument output layer\n",
    "        self.argument_outlayer = BottledXavierLinear(in_features=2*(hyps['tag_hidden']+hyps['tri_dim']+hyps['tri_size']),\n",
    "                                                     out_features=arguments_size).to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self,token_sequence,pos_taggig_sequence,entity_type_sequence,trigger_type_sequence,\n",
    "                adj,head_index,x_len,b_len,gold_triggers=None,gold_arguments=None,is_train=True):\n",
    "        '''\n",
    "        jointly extracte event trigger and argument\n",
    "        \n",
    "        :param token_sequence: LongTensor, padded word indices, (batch_size, bert_seq_len)\n",
    "        :param pos_tagging_sequence: LongTensor, padded pos-tagging label indices, (batch_size, seq_len)\n",
    "        :param entity_type_sequence: list, padded entity label indices keep all possible labels, (batch_size, seq_len, variable_length>=1)\n",
    "        :param trigger_type_sequence: LongTensor, padded trigger label indices, (batch_size, seq_len)\n",
    "        :param adj: sparse.FloatTensor, adjacent matrix for provided graph of padded sequences, (batch_size, edge_types, seq_len, seq_len)\n",
    "        :param head_index: LongTensor,including [CLS] and [SEP] representation,we give credits only to the first piece. (batch_size, seq_len)\n",
    "        :param x_len: numpy int64 array, indicating corresponding actual sequence length, (batch_size,)\n",
    "        :param b_len: numpy int64 array, indicating corresponding actual bert sequence length, (batch_size,)\n",
    "        :param gold_triggers: list, standerd trigger string label\n",
    "        :param gold_arguments: list, standard argument string label\n",
    "        '''\n",
    "        \n",
    "        BATCH_SIZE = head_index.shape[0]\n",
    "        SEQ_LEN = head_index.shape[1]\n",
    "        \n",
    "        mask = np.zeros(shape=(BATCH_SIZE,SEQ_LEN),dtype=np.uint8)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            s_len = x_len[i]\n",
    "            mask[i,0:s_len] = np.ones(shape=(s_len),dtype=np.uint8)\n",
    "        mask = torch.ByteTensor(mask).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_emb = self.bert(token_sequence)[0]\n",
    "\n",
    "            \n",
    "        word_emb = self.reconstruct_embedding(token_emb,head_index,BATCH_SIZE,SEQ_LEN,x_len,b_len,mode=self.hyperparams['bert_mode'])\n",
    "        pos_emd = self.posembedding(pos_taggig_sequence)\n",
    "        enti_emb = self.enembedding(entity_type_sequence)\n",
    "\n",
    "        rnn_in = torch.cat([word_emb,pos_emd,enti_emb],2)\n",
    "        rnn_in = self.BN(rnn_in)\n",
    "        x,_ = self.birnn(rnn_in,x_len)\n",
    "        x = self.BN(x)\n",
    "        \n",
    "        x1 = word_emb\n",
    "        for i in range(self.hyperparams['gcn_layers']):\n",
    "            x1 = self.gcns[i](x1,adj)\n",
    "            \n",
    "        x1 = self.BN(x1)\n",
    "        \n",
    "        x1,weight = self.atten(x1)\n",
    "        x1 = self.BN(x1)\n",
    "        \n",
    "        tagger_in = torch.cat([x,x1],2)\n",
    "        #tagger\n",
    "        x_tag,_ = self.tagger(tagger_in)\n",
    "        x_tag = self.BN(x_tag)\n",
    "        #decoder层\n",
    "      \n",
    "        trigger_loss,pred_trigger,argument_hidden,argument_keys = self.predict_triggers(x_tag,trigger_type_sequence,gold_arguments,mask,is_train=is_train)\n",
    "        return trigger_loss,pred_trigger,argument_hidden,argument_keys\n",
    "    \n",
    "        \n",
    "    def reconstruct_embedding(self,bert_emb,head_index,batch_size,seq_len,x_len,b_len,mode='first'):\n",
    "        '''\n",
    "        mode: str,有两种形式\"first\"和\"average\",first表示只是用bert向量的头token向量表示该词向量\n",
    "                                               average表示是用该单词的所有token向量平均表示词向量\n",
    "        '''\n",
    "        \n",
    "        word_emb = torch.empty(bert_emb.shape).to(self.device)\n",
    "        \n",
    "        if mode == 'first':\n",
    "            for i in range(batch_size):\n",
    "                word_emb[i] = torch.index_select(bert_emb[i],0,head_index[i])\n",
    "        else:\n",
    "            for i in range(batch_size):\n",
    "                real_len = x_len[i]\n",
    "                bert_len = b_len[i]\n",
    "                temp_token_emb = bert_emb[i]\n",
    "                temp_head_index = head_index[i]\n",
    "                for j in range(real_len):\n",
    "                    if j == real_len-1:\n",
    "                        t = temp_token_emb[temp_head_index[j]:bert_len-1].mean(dim=0)\n",
    "                        word_emb[i][j] = t\n",
    "                    else:\n",
    "                        t = temp_token_emb[temp_head_index[j]:temp_head_index[j+1]].mean(dim=0)\n",
    "                        word_emb[i][j] = t\n",
    "\n",
    "                for j in range(real_len,seq_len):\n",
    "                    word_emb[i][j] = temp_token_emb[-1]\n",
    "            \n",
    "        return word_emb\n",
    "    \n",
    "    def predict_triggers(self,inputs,trigger_type_sequence,gold_arguments,mask,is_train=True):\n",
    "        trigger_logits = self.trigger_outlayer(inputs)\n",
    "        #trigger_soft = F.log_softmax(trigger_logits+1e-10,dim=2)\n",
    "        trigger_soft = F.softmax(trigger_logits+1e-10,dim=2)\n",
    "        pred_trigger = torch.argmax(trigger_soft,dim=2)\n",
    "        \n",
    "        BATCH = trigger_soft.shape[0]\n",
    "        SEQ = trigger_soft.shape[1]\n",
    "        output_ = trigger_soft.view(BATCH*SEQ,-1)\n",
    "        label_ = trigger_type_sequence.view(BATCH*SEQ,-1)\n",
    "        mask = mask.view(BATCH*SEQ,)\n",
    "        mask_index = torch.LongTensor([x for x in range(BATCH*SEQ) if mask[x] == 1]).to(self.device)\n",
    "        \n",
    "        output_l = output_.index_select(0,mask_index)\n",
    "        label_l = label_.index_select(0,mask_index).squeeze(1)\n",
    "        \n",
    "        trigger_loss = dice_loss(output_l,label_l,alpha=0.3,loss_type='DSC')\n",
    "        \n",
    "        if is_train:\n",
    "            assert gold_arguments is not None\n",
    "            trigger_emb = self.triembedding(trigger_type_sequence)\n",
    "        else:\n",
    "            trigger_emb = self.triembedding(pred_trigger)\n",
    "\n",
    "        features = torch.cat([inputs,trigger_emb,trigger_soft],2)\n",
    "        \n",
    "        \n",
    "        argument_hidden = []\n",
    "        argument_keys = []\n",
    "        \n",
    "        for i in range(inputs.shape[0]):\n",
    "            candidates = gold_arguments[i]['candidates']\n",
    "            golden_candidate_tensor = {}\n",
    "            \n",
    "            for j in range(len(candidates)):\n",
    "                can_st,can_ed,can_type = candidates[j]\n",
    "                golden_candidate_tensor[candidates[j]] = features[i,can_st:can_ed, ].mean(dim=0)\n",
    "            \n",
    "            predicted_triggers = self.find_triggers([idx2trigger[t] for t in pred_trigger[i].tolist()])\n",
    "            for predicted_trigger in predicted_triggers:\n",
    "                t_start, t_end, t_type_str = predicted_trigger\n",
    "                event_tensor = features[i,t_start:t_end, ].mean(dim=0)\n",
    "                for j in range(len(candidates)):\n",
    "                    if predicted_trigger != candidates[j]:\n",
    "                        e_start, e_end, e_type_str = candidates[j]\n",
    "                        candidates_tensor = golden_candidate_tensor[candidates[j]]\n",
    "                        \n",
    "                        argument_hidden.append(torch.cat([event_tensor,candidates_tensor]))\n",
    "                        argument_keys.append((i,t_start, t_end, t_type_str,e_start, e_end, e_type_str))\n",
    "        \n",
    "        return trigger_loss,pred_trigger,argument_hidden,argument_keys\n",
    "        \n",
    "    def predict_arguments(self,argument_input,argument_keys,gold_arguments):\n",
    "       \n",
    "        argument_input = torch.stack(argument_input)\n",
    "        argument_input = argument_input.masked_fill(torch.isnan(argument_input),0)\n",
    "        argument_logits = self.argument_outlayer(argument_input)\n",
    "        argument_soft = F.softmax(argument_logits,dim=1)\n",
    "      \n",
    "        pred_argument = argument_soft.argmax(-1)\n",
    "        gold_argument_ids = []\n",
    "        for i,trigger_st,trigger_en,trigger_type_str,entity_st,entity_en,entity_type_str in argument_keys:\n",
    "            a_label = argument2idx[NONE]\n",
    "            if (trigger_st,trigger_en,trigger_type_str) in gold_arguments[i]['events']: #if event match\n",
    "                for (a_st,a_en,a_type_id) in gold_arguments[i]['events'][(trigger_st,trigger_en,trigger_type_str)]:\n",
    "                    if a_st == entity_st and a_en == entity_en:\n",
    "                        a_label = a_type_id\n",
    "                        break\n",
    "            gold_argument_ids.append(a_label)\n",
    "        gold_argument_ids = torch.LongTensor(gold_argument_ids).to(self.device)\n",
    "        \n",
    "        loss = dice_loss(argument_soft,gold_argument_ids,loss_type='DSC')\n",
    "        \n",
    "        pred_argument = pred_argument.view(gold_argument_ids.size()).tolist()\n",
    "        batch_size = len(gold_arguments)\n",
    "        pred_event = [{'events':{}} for _ in range(batch_size)]\n",
    "        for (i,t_st,t_ed,event_type_str,e_st,e_ed,entity_type),pred_label in zip(argument_keys,pred_argument):\n",
    "            if pred_label == argument2idx[NONE]:\n",
    "                continue\n",
    "\n",
    "            if (t_st,t_ed,event_type_str) not in pred_event[i]['events']:\n",
    "                pred_event[i]['events'][t_st,t_ed,event_type_str] = []\n",
    "            pred_event[i]['events'][t_st,t_ed,event_type_str].append((e_st,e_ed,pred_label))\n",
    "\n",
    "        return loss, pred_event\n",
    "        \n",
    "    def find_triggers(self,labels):\n",
    "        '''\n",
    "        :param labels: ['B-Conflict:Attack', 'I-Conflict:Attack', 'O', 'B-Life:Marry']\n",
    "        :return: [(0, 2, 'Conflict:Attack'), (3, 4, 'Life:Marry')]\n",
    "        '''\n",
    "        result = []\n",
    "        labels = [label.split('-',1) for label in labels]\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i][0] == 'B':\n",
    "                result.append([i, i + 1, labels[i][1]])\n",
    "\n",
    "        for item in result:\n",
    "            j = item[1]\n",
    "            while j < len(labels):\n",
    "                if labels[j][0] == 'I':\n",
    "                    j = j + 1\n",
    "                    item[1] = j\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return [tuple(item) for item in result]\n",
    "    \n",
    "    def save_model(self,path):\n",
    "        state_dict = self.state_dict()\n",
    "        for key,value in state_dict.items():\n",
    "            state_dict[key] = value.cpu()\n",
    "        torch.save(state_dict,path)\n",
    "    \n",
    "    def load_model(self,path):\n",
    "        state_dict = torch.load(path)\n",
    "        self.load_state_dict(state_dict)\n",
    "    \n",
    "    def __getnewargs__(self):\n",
    "        # for pickle\n",
    "        return self.hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NONE = 'O'\n",
    "PAD = \"[PAD]\"\n",
    "UNK = \"[UNK]\"\n",
    "\n",
    "# for BERT\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "\n",
    "#event triggers\n",
    "TRIGGERS = ['Dephosphorylation','Binding','Blood_vessel_development','Breakdown',\n",
    "            'Catabolism','Cell_proliferation','Death','Development','Gene_expression',\n",
    "            'Growth','Localization','Negative_regulation','Positive_regulation',\n",
    "            'Phosphorylation','Planned_process','Regulation','Remodeling','Synthesis',\n",
    "            'Transcription']\n",
    "\n",
    "#argument roles\n",
    "ARGUMENTS = ['Cause','Theme','Theme2','NONE']\n",
    "\n",
    "#entities\n",
    "ENTITIES = ['Anatomical_system', 'Cell', 'Cellular_component',\n",
    "            'DNA_domain_or_region', 'Developing_anatomical_structure',\n",
    "            'Drug_or_compound', 'Gene_or_gene_product', 'Immaterial_anatomical_entity',\n",
    "            'Multi-tissue_structure','Organ','Organism','Organism_subdivision',\n",
    "            'Organism_substance','Pathological_formation','Protein_domain_or_region','Tissue']\n",
    "\n",
    "# POS tags\n",
    "POSTAGS = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ',\n",
    "           'DET', 'INTJ', 'NOUN', 'NUM', 'PART',\n",
    "           'PRON', 'PROPN', 'PUNCT', 'SCONJ',\n",
    "           'SYM', 'VERB', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(labels, BIO_tag=False):\n",
    "    '''\n",
    "    building vocab by label list\n",
    "    labels: list, all golden label type needed\n",
    "    BIO_tag: default False, if True, we'll add BIO tag in each label in label list\n",
    "    '''\n",
    "    all_labels = []\n",
    "    if BIO_tag:\n",
    "        B_list = ['B-{}'.format(label) for label in labels]\n",
    "        I_list = ['I-{}'.format(label) for label in labels]\n",
    "        all_labels = [PAD,NONE]+B_list+I_list\n",
    "    else:\n",
    "        all_labels = [PAD,NONE]+labels\n",
    "    \n",
    "    label2idx = {tag:idx for idx,tag in enumerate(all_labels)}\n",
    "    idx2label = {idx:tag for idx,tag in enumerate(all_labels)}\n",
    "    vocab_size = len(all_labels)\n",
    "    return vocab_size,all_labels,label2idx,idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize related vocabulary\n",
    "triggers_size,all_triggers,trigger2idx,idx2trigger = build_vocab(labels=TRIGGERS,BIO_tag=True)\n",
    "arguments_size,all_arguments,argument2idx,idx2argument = build_vocab(labels=ARGUMENTS)\n",
    "entities_size,all_entities,entity2idx,idx2entity = build_vocab(labels=ENTITIES,BIO_tag=True)\n",
    "POS_size,all_POS,POS2idx,idx2POS = build_vocab(labels=POSTAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased',do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLEEDataset(data.Dataset):\n",
    "    def __init__(self,data_path):\n",
    "        self.textlist = []\n",
    "        self.is_headlist = []\n",
    "        self.entitylist = []\n",
    "        self.poslist = []\n",
    "        self.triggerlist = []\n",
    "        self.argumentlist = []\n",
    "        self.graph = []\n",
    "        self.acture_len = [] #the length of real word sequence\n",
    "        self.acture_bert_len = [] #the length of real bert token(after bert participle)\n",
    "        self.maxlen = 64\n",
    "        \n",
    "        with codecs.open(data_path,mode='r',encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                text,is_head,seqlen,bseqlen = self.getTextlist(item['tokens'])\n",
    "                entities = self.getEntitylist(item['gold_entity_mentions'],seqlen)\n",
    "                pos = self.getPoslist(item['pos-tags'],seqlen)\n",
    "                triggers,arguments = self.getTriandArglist(item['gold_event_mentions'],item['gold_entity_mentions'],seqlen)\n",
    "                adjpos,adjv = self.getAdjMatrix(item['parse'],seqlen)\n",
    "                \n",
    "                self.textlist.append([CLS]+text+[SEP])\n",
    "                self.is_headlist.append([0]+is_head+[0])\n",
    "                self.entitylist.append(entities)\n",
    "                self.poslist.append(pos)\n",
    "                self.triggerlist.append(triggers)\n",
    "                self.argumentlist.append(arguments)\n",
    "                self.graph.append((adjpos,adjv))\n",
    "                self.acture_len.append(seqlen)\n",
    "                self.acture_bert_len.append(bseqlen)\n",
    "        \n",
    "    def getTextlist(self,texts):\n",
    "        #设置一个is_head的list，其作用有两个，第一可以计算bert分词以后，在设置完bert的要求后，真正的输入序列长度是多少\n",
    "        #第二就是可以标记分词以后的头token，比如apples切分完以后为 apple,##s,实际参与运算的是apple向量，所以我们需要一个head标记\n",
    "        is_head = []\n",
    "        tokens = []\n",
    "        for word in texts:\n",
    "            token_li = tokenizer.tokenize(word)\n",
    "            head = [1] + (len(token_li)-1)*[0]\n",
    "            \n",
    "            tokens.extend(token_li)\n",
    "            is_head.extend(head)\n",
    "            \n",
    "\n",
    "        #由于bert可以处理的最长序列长度为512，另外bert需要添加cls和sep特殊标志，所以超过的长度需要减2\n",
    "        if len(tokens)>=self.maxlen-1:\n",
    "            tokens = tokens[0:self.maxlen-2]\n",
    "            is_head = is_head[0:self.maxlen-2]\n",
    "\n",
    "        real_seqlen = 0 #用于计算序列中还村子的真正原始单词序列长度，而不是分词后的长度\n",
    "        for i in is_head:\n",
    "            real_seqlen=real_seqlen+i #本实验，用first token代表整个单词，所以有多少个first token,则表示有多少个单词\n",
    "        \n",
    "        real_bert_sqlen = len(tokens)+2\n",
    "\n",
    "        return tokens,is_head,real_seqlen,real_bert_sqlen\n",
    "        \n",
    "    def getEntitylist(self,entityjson,length):\n",
    "        en_list = [[NONE] for _ in range(length)]\n",
    "        for entity_mention in entityjson:\n",
    "            start = entity_mention['start']\n",
    "            end = entity_mention['end']\n",
    "            for i in range(start,end):\n",
    "                entity_type = entity_mention['entity_type']\n",
    "                if i >=length:\n",
    "                    break\n",
    "                else:\n",
    "                    if i==start:\n",
    "                        entity_type = 'B-{}'.format(entity_type)\n",
    "                    else:\n",
    "                        entity_type = 'I-{}'.format(entity_type)\n",
    "\n",
    "                    if len(en_list[i])==1 and en_list[i][0]==NONE:\n",
    "                        en_list[i][0]=entity_type\n",
    "                    else:\n",
    "                        en_list[i].append(entity_type)\n",
    "        return en_list\n",
    "        \n",
    "    def getPoslist(self,posjson,length):\n",
    "        return posjson[0:length]\n",
    "        \n",
    "    def getTriandArglist(self,eventjson,entityjson,length):\n",
    "        '''\n",
    "        通过所给的json格式的event和entity，分别获取相应的trigger标注和argument标注\n",
    "        其中，trigger标注格式为['B-event_type','I-event_type',...]\n",
    "        argument标注格式为{\n",
    "                            'candidates':[(start,end,entity_type),...],\n",
    "                            'events':{(start,end,event_type):[(start,end,argument_role),...]}\n",
    "                            }\n",
    "        '''\n",
    "\n",
    "        tri_list = [NONE]*length\n",
    "        arg_dic = {\n",
    "            'candidates':[],\n",
    "            'events':{}\n",
    "        }\n",
    "\n",
    "        temp_candidates_for_tri = []\n",
    "        for event_mention in eventjson:\n",
    "            tri_start = event_mention['trigger']['start']\n",
    "            tri_end = event_mention['trigger']['end']\n",
    "            tri_type = event_mention['event_type']\n",
    "\n",
    "            for i in range(tri_start,tri_end):\n",
    "                type_ = tri_type\n",
    "                if i >=length:\n",
    "                    break\n",
    "                else:\n",
    "                    if i == tri_start:\n",
    "                        tri_list[i] = 'B-{}'.format(type_)\n",
    "                    else:\n",
    "                        tri_list[i] = 'I-{}'.format(type_)\n",
    "\n",
    "            event_key = (tri_start,tri_end,tri_type)\n",
    "            temp_candidates_for_tri.append(event_key)\n",
    "            event_value = []\n",
    "            if len(event_mention['arguments']) == 0:\n",
    "                arg_start = -1\n",
    "                arg_end = -1\n",
    "                arg_role = 'NONE'\n",
    "                event_value.append((arg_start,arg_end,argument2idx[arg_role]))\n",
    "            else:\n",
    "                for argument_mention in event_mention['arguments']:\n",
    "                    arg_start = argument_mention['start']\n",
    "                    arg_end = argument_mention['end']\n",
    "                    arg_role = argument_mention['role']\n",
    "                    event_value.append((arg_start,arg_end,argument2idx[arg_role]))\n",
    "            if event_key in arg_dic['events'].keys():\n",
    "                event_value.extend(arg_dic['events'][event_key])\n",
    "                new_event_value = list(set(event_value))\n",
    "                arg_dic['events'][event_key] = list(sorted(new_event_value))\n",
    "                \n",
    "            else:\n",
    "                arg_dic['events'][event_key]=list(sorted(event_value))\n",
    "\n",
    "        arg_dic['candidates'].extend(temp_candidates_for_tri)\n",
    "        for entity_mention in entityjson:\n",
    "            entity_start = entity_mention['start']\n",
    "            entity_end = entity_mention['end']\n",
    "            entity_type = entity_mention['entity_type']\n",
    "            arg_dic['candidates'].append((entity_start,entity_end,entity_type))\n",
    "        arg_dic['candidates'].append((-1,-1,'NONE')) #MLEE语料中存在“事件中没有argument的情况，对于该情况，我们统一使用(-1,-1,'NONE')表示”\n",
    "\n",
    "        return tri_list,arg_dic\n",
    "        \n",
    "    def getAdjMatrix(self,parsejson,length):\n",
    "        sparseAdjMatrixPos = [[], [], []]\n",
    "        sparseAdjMatrixValues = []\n",
    "\n",
    "        def addedge(type_, from_, to_, value_):\n",
    "            sparseAdjMatrixPos[0].append(type_)\n",
    "            sparseAdjMatrixPos[1].append(from_)\n",
    "            sparseAdjMatrixPos[2].append(to_)\n",
    "            sparseAdjMatrixValues.append(value_)\n",
    "\n",
    "        for edge in parsejson:\n",
    "            temp = edge.strip().split('/')\n",
    "            from_ = int(temp[2].split('=')[-1])\n",
    "            to_ = int(temp[1].split('=')[-1])\n",
    "            etype = temp[0]\n",
    "            if etype == 'root' or from_ == -1 or to_ == -1 or from_ >= length or to_ >= length:\n",
    "                continue\n",
    "            addedge(0,from_,to_,1.0)\n",
    "            addedge(1,to_,from_,1.0)\n",
    "\n",
    "        for i in range(length):\n",
    "            addedge(2,i,i,1.0)\n",
    "\n",
    "        return sparseAdjMatrixPos,sparseAdjMatrixValues\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.textlist)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        texts,is_heads,entities,poses,triggers,arguments,graphes,x_len,b_len = self.textlist[idx],self.is_headlist[idx],\\\n",
    "                                                                   self.entitylist[idx],self.poslist[idx],self.triggerlist[idx],\\\n",
    "                                                                   self.argumentlist[idx],self.graph[idx],self.acture_len[idx],\\\n",
    "                                                                   self.acture_bert_len[idx]\n",
    "\n",
    "        input_texts_ids = tokenizer.convert_tokens_to_ids(texts)\n",
    "        input_entities_ids = [[entity2idx[e] for e in entitys] for entitys in entities]\n",
    "        input_poses_ids = [POS2idx[p] for p in poses]\n",
    "        trigger_y_ids = [trigger2idx[t] for t in triggers]\n",
    "\n",
    "        head_index = []\n",
    "        for index,tag in enumerate(is_heads):\n",
    "            if tag:\n",
    "                head_index.append(index)\n",
    "\n",
    "        return input_texts_ids,input_entities_ids,input_poses_ids,trigger_y_ids,\\\n",
    "                head_index,x_len,b_len,triggers,arguments,graphes\n",
    "        \n",
    "    def get_samples_weight(self):\n",
    "        samples_weight = []\n",
    "        for triggers in self.triggerlist:\n",
    "            not_none = False\n",
    "            for trigger in triggers:\n",
    "                if trigger != NONE:\n",
    "                    not_none = True\n",
    "                    break\n",
    "            if not_none:\n",
    "                samples_weight.append(8.0)\n",
    "            else:\n",
    "                samples_weight.append(0.5)\n",
    "        return np.array(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    input_ids_2d,entity_ids_3d,pos_ids_2d,trigger_ids_2d,head_index_2d,x_len_1d,b_len_1d,triggers_2d,arguments_2d,graph_3d = list(map(list,zip(*batch)))\n",
    "    maxlen = 64\n",
    "    for i in range(len(input_ids_2d)):\n",
    "        input_ids_2d[i] = input_ids_2d[i] + [0]*(maxlen-len(input_ids_2d[i]))\n",
    "        entity_ids_3d[i] = entity_ids_3d[i] + [[entity2idx[PAD]] for _ in range(maxlen-len(entity_ids_3d[i]))]\n",
    "        pos_ids_2d[i] = pos_ids_2d[i] + [POS2idx[PAD]]*(maxlen-len(pos_ids_2d[i]))\n",
    "        trigger_ids_2d[i] = trigger_ids_2d[i] + [trigger2idx[PAD]]*(maxlen-len(trigger_ids_2d[i]))\n",
    "        head_index_2d[i] = head_index_2d[i] + [63]*(maxlen-len(head_index_2d[i]))\n",
    "        \n",
    "    x_len_1d = np.array(x_len_1d,dtype=np.int64)\n",
    "    b_len_1d = np.array(b_len_1d,dtype=np.int64)\n",
    "    \n",
    "    return input_ids_2d,entity_ids_3d,pos_ids_2d,\\\n",
    "            trigger_ids_2d,head_index_2d,x_len_1d,\\\n",
    "            b_len_1d,triggers_2d,arguments_2d,graph_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_li = []\n",
    "dev_loss_li = []\n",
    "test_loss_li = []\n",
    "\n",
    "train_f = []\n",
    "dev_f = []\n",
    "test_f = []\n",
    "\n",
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_dataset,dev_dataset,test_dataset,optimizer,run_perparam):\n",
    "    samples_weight = train_dataset.get_samples_weight()\n",
    "    sampler = data.WeightedRandomSampler(samples_weight,len(samples_weight))\n",
    "    \n",
    "    train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=run_perparam['train_batch'],\n",
    "                                 shuffle=False,\n",
    "                                 sampler=sampler,\n",
    "                                 num_workers=10,\n",
    "                                 collate_fn=pad)\n",
    "    dev_iter = data.DataLoader(dataset=dev_dataset,\n",
    "                               batch_size=run_perparam['dev_batch'],\n",
    "                               shuffle=True,\n",
    "                               collate_fn=pad)\n",
    "    test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=run_perparam['test_batch'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=pad)\n",
    "    \n",
    "    best_scores = 0.0\n",
    "    keep_best = 0\n",
    "    \n",
    "    \n",
    "    epochs = run_perparam['epoch']\n",
    "    print('training beginning.......')\n",
    "    scheduler = StepLR(optimizer=optimizer,step_size=15,gamma=0.1,last_epoch=-1)\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        #training\n",
    "        print(\"Epoch\",i+1)\n",
    "        training_loss,training_trigger_acc,training_trigger_p,training_trigger_r,training_trigger_f,\\\n",
    "        training_argument_p,training_argument_r,training_argument_f = run_over_data(data_iter = train_iter,\n",
    "                                                                                    optimizer = optimizer,\n",
    "                                                                                    total = math.ceil(len(train_dataset)/run_perparam['train_batch']),\n",
    "                                                                                    model = model,\n",
    "                                                                                    need_backward = True,\n",
    "                                                                                    hyps = model.hyperparams,\n",
    "                                                                                    device = model.device,\n",
    "                                                                                    save_output = os.path.join(run_perparam['out'],\n",
    "                                                                                                            'training_epoch_%d.txt'%(i+1)))\n",
    "        \n",
    "        print(\"\\nEpoch\", i + 1, \" training loss: \", training_loss,\n",
    "              \"\\ntraining trigger Acc: \",training_trigger_acc,\n",
    "              \"\\ntraining trigger p: \", training_trigger_p,\n",
    "              \" training trigger r: \", training_trigger_r,\n",
    "              \" training trigger f1: \", training_trigger_f,\n",
    "              \"\\ntraining argument p: \", training_argument_p,\n",
    "              \" training argument r: \", training_argument_r,\n",
    "              \" training argument f1: \", training_argument_f)\n",
    "        \n",
    "        train_loss_li.append(training_loss)\n",
    "        train_f.append(training_trigger_f)\n",
    "        #validation\n",
    "        dev_loss,dev_trigger_acc,dev_trigger_p,dev_trigger_r,dev_trigger_f,\\\n",
    "        dev_argument_p,dev_argument_r,dev_argument_f = run_over_data(data_iter = dev_iter,\n",
    "                                                                     optimizer = optimizer,\n",
    "                                                                     total = math.ceil(len(dev_dataset)/run_perparam['dev_batch']),\n",
    "                                                                     model = model,\n",
    "                                                                     need_backward = False,\n",
    "                                                                     hyps = model.hyperparams,\n",
    "                                                                     device = model.device,\n",
    "                                                                     save_output = os.path.join(run_perparam['out'],\n",
    "                                                                                                            'dev_epoch_%d.txt'%(i+1)))\n",
    "        \n",
    "        print(\"\\nEpoch\", i + 1, \" dev loss: \", dev_loss,\n",
    "              \"\\ndev trigger Acc: \",dev_trigger_acc,\n",
    "              \"\\ndev trigger p: \", dev_trigger_p,\n",
    "              \" dev trigger r: \", dev_trigger_r,\n",
    "              \" dev trigger f1: \", dev_trigger_f,\n",
    "              \"\\ndev argument p: \", dev_argument_p,\n",
    "              \" dev argument r: \", dev_argument_r,\n",
    "              \" dev argument f1: \", dev_argument_f)\n",
    "        dev_loss_li.append(dev_loss)\n",
    "        dev_f.append(dev_trigger_f)\n",
    "        \n",
    "        #test\n",
    "        test_loss,test_trigger_acc,test_trigger_p,test_trigger_r,test_trigger_f,\\\n",
    "        test_argument_p,test_argument_r,test_argument_f = run_over_data(data_iter = test_iter,\n",
    "                                                                        optimizer = optimizer,\n",
    "                                                                        total = math.ceil(len(test_dataset)/run_perparam['test_batch']),\n",
    "                                                                        model = model,\n",
    "                                                                        need_backward = False,\n",
    "                                                                        hyps = model.hyperparams,\n",
    "                                                                        device = model.device,\n",
    "                                                                        save_output = os.path.join(run_perparam['out'],\n",
    "                                                                                                            'test_epoch_%d.txt'%(i+1)))\n",
    "        \n",
    "        print(\"\\nEpoch\", i + 1, \" test loss: \", test_loss,\n",
    "              \"\\ntest trigger Acc: \",test_trigger_acc,\n",
    "              \"\\ntest trigger p: \", test_trigger_p,\n",
    "              \" test trigger r: \", test_trigger_r,\n",
    "              \" test trigger f1: \", test_trigger_f,\n",
    "              \"\\ntest argument p: \", test_argument_p,\n",
    "              \" test argument r: \", test_argument_r,\n",
    "              \" test argument f1: \", test_argument_f)\n",
    "        \n",
    "        test_loss_li.append(test_loss)\n",
    "        test_f.append(test_trigger_f)\n",
    "        \n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "        scheduler.step()\n",
    "        #early stop\n",
    "   \n",
    "        if best_scores <= dev_trigger_f+dev_argument_f:\n",
    "            best_scores = dev_trigger_f+dev_argument_f\n",
    "            model.save_model(os.path.join(run_perparam['out'],'model.pt'))\n",
    "            print('save the model on CPU at epoch ',i+1)\n",
    "            keep_best = 0\n",
    "        \n",
    "        else:\n",
    "            keep_best = keep_best+1\n",
    "            if keep_best >= run_perparam['early_stop']:\n",
    "                print('stopping training, best model is loaded')\n",
    "                model.load_model(os.path.join(run_perparam['out'],'model.pt'))\n",
    "                break\n",
    "        \n",
    "        \n",
    "    #finnal test \n",
    "    test_loss,test_trigger_acc,test_trigger_p,test_trigger_r,test_trigger_f,\\\n",
    "    test_argument_p,test_argument_r,test_argument_f = run_over_data(data_iter = test_iter,\n",
    "                                                                    optimizer = optimizer,\n",
    "                                                                    total = math.ceil(len(test_dataset)/run_perparam['test_batch']),\n",
    "                                                                    model = model,\n",
    "                                                                    need_backward = False,\n",
    "                                                                    hyps = model.hyperparams,\n",
    "                                                                    device = model.device,\n",
    "                                                                    save_output = os.path.join(run_perparam['out'],\n",
    "                                                                                               'test_final.txt'))\n",
    "    print(\"\\nFinally test loss: \", test_loss,\n",
    "          \"\\ntest trigger Acc: \",test_trigger_acc,\n",
    "          \"\\ntest trigger p: \", test_trigger_p,\n",
    "          \" test trigger r: \", test_trigger_r,\n",
    "          \" test trigger f1: \", test_trigger_f,\n",
    "          \"\\ntest argument p: \", test_argument_p,\n",
    "          \" test argument r: \", test_argument_r,\n",
    "          \" test argument f1: \", test_argument_f)\n",
    "    \n",
    "    print('training endding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_over_data(data_iter,optimizer,total,model,need_backward,hyps,device,save_output):\n",
    "    if need_backward:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    all_tri = [] #golden trigger label \n",
    "    all_tri_ = [] #predicted trigger label\n",
    "    all_event = [] #golden event \n",
    "    all_event_ = [] #predicted event\n",
    "    all_tokens = []\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    cnt = 0\n",
    "           \n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for batch in data_iter:\n",
    "        is_train = False\n",
    "        if need_backward:\n",
    "            optimizer.zero_grad()\n",
    "            is_train = True\n",
    "        \n",
    "        input_ids,entity_ids,pos_ids,\\\n",
    "        trigger_label_ids,head_index,x_len,\\\n",
    "        b_len,gold_triggers,gold_arguments,graphs = batch\n",
    "        \n",
    "        input_tensor = torch.LongTensor(input_ids)\n",
    "        pos_tensor = torch.LongTensor(pos_ids)\n",
    "        trigger_label = torch.LongTensor(trigger_label_ids)\n",
    "        head_index_tensor = torch.LongTensor(head_index)\n",
    "        SEQ_LEN = head_index_tensor.size()[1]\n",
    "        adjm = torch.stack([torch.sparse.FloatTensor(torch.LongTensor(graph[0]),\n",
    "                                                     torch.FloatTensor(graph[1]),\n",
    "                                                     torch.Size((hyps['gcn_et'],SEQ_LEN,SEQ_LEN))).to_dense() for graph in graphs])\n",
    "        \n",
    "        \n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        pos_tensor = pos_tensor.to(device)\n",
    "        trigger_label = trigger_label.to(device)\n",
    "        head_index_tensor = head_index_tensor.to(device)\n",
    "        adjm = adjm.to(device)\n",
    "        \n",
    "        trigger_loss,pred_triggers,argument_input,argument_keys = model.forward(token_sequence = input_tensor,\n",
    "                                                                                  pos_taggig_sequence = pos_tensor,\n",
    "                                                                                  entity_type_sequence = entity_ids,\n",
    "                                                                                  trigger_type_sequence = trigger_label,\n",
    "                                                                                  adj = adjm,\n",
    "                                                                                  head_index = head_index_tensor,\n",
    "                                                                                  x_len = x_len,\n",
    "                                                                                  b_len = b_len,\n",
    "                                                                                  gold_triggers = gold_triggers,\n",
    "                                                                                  gold_arguments = gold_arguments,\n",
    "                                                                                  is_train = is_train)\n",
    "        \n",
    "       \n",
    "\n",
    "        loss = trigger_loss\n",
    "  \n",
    "        if len(argument_keys) > 0:\n",
    "            argument_loss, pred_events = model.predict_arguments(argument_input = argument_input,\n",
    "                                                                 argument_keys = argument_keys,\n",
    "                                                                 gold_arguments = gold_arguments)\n",
    "            loss = trigger_loss+2*argument_loss\n",
    "        else:\n",
    "            loss = trigger_loss\n",
    "            pred_events = [{'events':{}} for _ in range(head_index_tensor.size()[0])]\n",
    "        \n",
    "        \n",
    "        all_event_.extend(pred_events)\n",
    "        \n",
    "        true_events = []\n",
    "        for gold_argument in gold_arguments:\n",
    "            event_val = gold_argument['events']\n",
    "            true_events.append({'events':event_val})\n",
    "        all_event.extend(true_events)\n",
    "        \n",
    "    \n",
    "        pred_triggers = pred_triggers.view(trigger_label.size()).tolist()\n",
    "        true_triggers = trigger_label.tolist()\n",
    "        \n",
    "        def clear_tag(list_):\n",
    "            r = ''\n",
    "            for t in list_:\n",
    "                temp = t\n",
    "                if temp.startswith('##'):\n",
    "                    temp = temp[2:]\n",
    "                r = r+temp\n",
    "            return r\n",
    "        \n",
    "        def addtokens(input_tokens_ids,true_label_ids,pred_label_ids,x_len,b_len,head_index):\n",
    "            all_tokens = []\n",
    "            for i in range(len(input_tokens_ids)):\n",
    "                input_tokens = tokenizer.convert_ids_to_tokens(input_tokens_ids[i])\n",
    "                real_len = x_len[i]\n",
    "                SEP_index = b_len[i]-1\n",
    "                true_label = true_label_ids[i][:real_len]\n",
    "                pred_label = pred_label_ids[i][:real_len]\n",
    "                head_i = head_index[i][:real_len]\n",
    "                words = []\n",
    "                for i in range(real_len):\n",
    "                    if i == real_len-1:\n",
    "                        tokens = input_tokens[head_i[i]:SEP_index]\n",
    "                        words.append(clear_tag(tokens))\n",
    "                    else:\n",
    "                        tokens = input_tokens[head_i[i]:head_i[i+1]]\n",
    "                        words.append(clear_tag(tokens))\n",
    "                \n",
    "                atoken = []\n",
    "                for w,tl,pl in zip(words,true_label,pred_label):\n",
    "                    atoken.append((w,idx2trigger[tl],idx2trigger[pl]))\n",
    "                all_tokens.append(atoken)\n",
    "            return all_tokens\n",
    "        \n",
    "        all_tokens.extend(addtokens(input_ids,true_triggers,pred_triggers,x_len,b_len,head_index))\n",
    "        \n",
    "        for index,l in enumerate(x_len):\n",
    "            pred_triggers[index] = pred_triggers[index][:l]\n",
    "            true_triggers[index] = true_triggers[index][:l]\n",
    "        \n",
    "        true_triggers_strs = [idx2trigger[t] for trigger in true_triggers for t in trigger]\n",
    "        pred_triggers_strs = [idx2trigger[t] for trigger in pred_triggers for t in trigger]\n",
    "        p,r,f,acc = getEval(true_triggers_strs,pred_triggers_strs,average='macro')\n",
    "        \n",
    "        all_tri.append(true_triggers_strs)\n",
    "        all_tri_.append(pred_triggers_strs)\n",
    "        \n",
    "        cnt += 1\n",
    "        other_info = ''\n",
    "        \n",
    "        if need_backward:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            other_info = 'Iter[{}] loss: {:.4f} TAcc: {:.2f}% TP: {:.2f}% TR: {:.2f}% TF1: {:.2f}%'.format(cnt, loss.item(),\n",
    "                                                                                                               acc * 100.0,\n",
    "                                                                                                               p * 100.0,\n",
    "                                                                                                               r * 100.0,\n",
    "                                                                                                               f * 100.0)\n",
    "        progressbar(cnt,total,other_info)\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if save_output:\n",
    "        with codecs.open(save_output, \"w\", encoding=\"utf-8\") as f:\n",
    "            for tokens in all_tokens:\n",
    "                for token in tokens:\n",
    "                    f.write(\"%s %s %s\\n\" % (token[0],token[1],token[2]))\n",
    "                f.write('\\n')\n",
    "    \n",
    "    running_loss = running_loss/cnt\n",
    "    tp,tr,rf,acc = getEval(all_tri,all_tri_,average='macro')\n",
    "    ep,er,ef = getEventEval(all_event,all_event_)\n",
    "\n",
    "    \n",
    "    return running_loss,acc,tp,tr,rf,ep,er,ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEval(true_label,pred_label,average=None):\n",
    "    '''\n",
    "    true_label:list\n",
    "    pred_label:list\n",
    "    average:str，[None,'micro','macro']\n",
    "    '''\n",
    "    \n",
    "    p = precision_score(true_label,pred_label,average=average)\n",
    "    r = recall_score(true_label,pred_label,average=average)\n",
    "    f1 = f1_score(true_label,pred_label,average=average)\n",
    "    acc = accuracy_score(true_label,pred_label)\n",
    "    \n",
    "    return p,r,f1,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEventEval(true_label,pred_label):\n",
    "    ct,p1,p2 = 0,0,0\n",
    "    for event,event_ in zip(true_label,pred_label):\n",
    "        true_event = event['events']\n",
    "        pred_event = event_['events']\n",
    "        for key,value in true_event.items():\n",
    "            p1 +=len(value)\n",
    "            if key not in pred_event:\n",
    "                continue\n",
    "            arguments = value\n",
    "            arguments_ = pred_event[key]\n",
    "            for temp,temp_ in zip(arguments,arguments_):\n",
    "                if temp[2] == temp_[2]:\n",
    "                    ct += 1\n",
    "        \n",
    "        for key,value in pred_event.items():\n",
    "            p2 += len(value)\n",
    "        \n",
    "        if ct == 0 or p1 == 0 or p2 == 0:\n",
    "            return 0.0,0.0,0.0\n",
    "        else:\n",
    "            p = 1.0 * ct / p2\n",
    "            r = 1.0 * ct / p1\n",
    "            f1 = 2.0 * p * r / (p + r)\n",
    "            return p,r,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(current,total,other_info):\n",
    "    percent = '{:.2%}'.format(current / total)\n",
    "    if type(other_info) is str:\n",
    "        print(\"\\r[%-50s] %s %s\" % ('=' * int(math.floor(current * 50 / total)), percent, other_info))\n",
    "    else:\n",
    "        print(\"\\r[%-50s] %s\" % ('=' * int(math.floor(current * 50 / total)), percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operating parameters\n",
    "run_param = {\n",
    "    'train_batch' : 64,              #train batch size\n",
    "    'dev_batch' : 16,               #dev batch size\n",
    "    'test_batch' : 16,              #test batch size\n",
    "    'epoch' : 200,                 #int\n",
    "    'lr' : 1e-1,                  #learning rate fload\n",
    "    'decay': 0.25,               #L2，float\n",
    "    'optimizer' : \"sgd\",         #Optimization，default is \"adam\"，others are \"adadelta\",\"sgd\" and \"adagrad\"\n",
    "    'out' : \"./out\",               #save path\n",
    "    'train_set' : \"train.json\",     #train set path\n",
    "    'dev_set' : \"dev.json\",           #dev set path\n",
    "    'test_set': \"test.json\",           #test set path\n",
    "    'early_stop' : 50,       \n",
    "    'device' : \"cuda\"          #\"cpu\" or \"cuda\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "hyps = {\n",
    "    'pos_size' : POS_size,\n",
    "    'posemd_dim' : 64,\n",
    "    'pos_dp' : 0.5,\n",
    "    'en_size' : entities_size,     #entity type numbers\n",
    "    'enemb_dim' : 64,              #multiEntity layer numbers\n",
    "    'en_dp' : 0.5,                 #multiEntity layer drop out\n",
    "    'rnn_dim' : 128,               #bidirectional rnn layer hidden numbers\n",
    "    'rnn_layers' : 2,              #bidirectional rnn layer  numbers\n",
    "    'rnn_dp' : 0.0,                #bidirectional rnn layer drop out\n",
    "    'rnn_mode' : \"lstm\",           #type of rnn，default is \"lstm\",others are \"gru\" and \"rnn\"\n",
    "    'gcn_layers' : 2,              #gcn layer numbers\n",
    "    'gcn_et' : 3,                  #edge types\n",
    "    'gcn_dp' : 0.5,                #gcn layer drop out\n",
    "    'gcn_use_bn' : True,          #gcn layer bais\n",
    "    'tag_hidden' : 128,           #tag layer hidden numbers\n",
    "    'tri_size' : triggers_size,   #trigger type numbers\n",
    "    'tri_dim' : 64,               #trigger dim\n",
    "    'decode_dp' : 0.5,            #decode layer drop out\n",
    "    'bert_mode': \"average\"        #bert vector representation，default is \"first\"，other is \"average\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BGModel(hyps=hyps,device=torch.device(run_param['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param['device'] == 'cuda':\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MLEEDataset(run_param['train_set'])\n",
    "dev_dataset = MLEEDataset(run_param['dev_set'])\n",
    "test_dataset = MLEEDataset(run_param['test_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),lr=run_param['lr'],weight_decay=run_param['decay'])\n",
    "elif run_param['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(),lr=run_param['lr'],weight_decay=run_param['decay'])\n",
    "elif run_param['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(params=model.parameters(),lr=run_param['lr'],weight_decay=run_param['decay'])\n",
    "else:\n",
    "    optimizer = torch.optim.Adagrad(params=model.parameters(),lr=run_param['lr'],weight_decay=run_param['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model,\n",
    "      train_dataset=train_dataset,\n",
    "      dev_dataset=dev_dataset,\n",
    "      test_dataset=test_dataset,\n",
    "      optimizer=optimizer,\n",
    "      run_perparam=run_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,200)\n",
    "y1 = train_loss_li\n",
    "y2 = dev_loss_li\n",
    "y3 = test_loss_li\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x,y1)\n",
    "plt.title('train loss vs epoch')\n",
    "plt.ylabel('train loss')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x,y2)\n",
    "plt.title('dev loss vs epoch')\n",
    "plt.ylabel('dev loss')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x,y3)\n",
    "plt.title('test loss vs epoch')\n",
    "plt.ylabel('test loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,200)\n",
    "y1 = train_f\n",
    "y2 = dev_f\n",
    "y3 = test_f\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x,y1)\n",
    "plt.title('train f vs epoch')\n",
    "plt.ylabel('train f')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x,y2)\n",
    "plt.title('dev f vs epoch')\n",
    "plt.ylabel('dev f')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x,y3)\n",
    "plt.title('test f vs epoch')\n",
    "plt.ylabel('test f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,100)\n",
    "z = lrs\n",
    "\n",
    "plt.plot(x,z)\n",
    "plt.title('LR vs epoch')\n",
    "plt.ylabel('LR')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
